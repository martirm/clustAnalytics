---
title: "test-vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{test-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  comment = "#>"
)
```

```{r setup}
library(clustAnalytics)
```


## Significance
To showcase the randomization process, we apply it to the Zachary's karate club graph, with the default settings (positive weights with no upper bound, which suits this graph):
```{r}
data(karate, package="igraphdata")
E(karate)
rewired_karate <- rewireCpp(karate, weight_sel = "max_weight")
E(rewired_karate)
```

Now we compute the scoring functions for the karate club graph. By default the clustering algorithms are Louvain, label propagation and Walktrap, but the function can take any list of clustering algorithms for igraph graphs.
```{r}
# this corresponds to the club each member ended up with after the split, 
# which we could consider the ground truth clustering for this graph.
karate_gt_clustering <- c(1,1,1,1,1,1,1,1,1,2,1,1,1,1,2,2,1,1,
                          2,1,2,1,2,2,2,2,2,2,2,2,2,2,2,2)
significance_table_karate <- evaluate_significance(karate, ground_truth=TRUE, 
                                                   gt_clustering=karate_gt_clustering)
significance_table_karate
```

Now we generate a graph from a stochastic block model in which we set very strong clusters (the elements in the diagonal of the matrix are much larger than the rest, so the probability of intra-cluster edges is much higher than that of inter-cluster edges).

```{r}
pm <- matrix (c(.3, .001, .001, .003,
                .001, .2, .005, .002,
                .001, .005, .2, .001,
                .003, .002, .001, .3), nrow=4, ncol=4)
g_sbm <- sample_sbm(100, pref.matrix=pm, block.sizes=c(25,25,25,25))
E(g_sbm)$weight <- 1
significance_table_sbm <- evaluate_significance(g_sbm)
significance_table_sbm
```


## Stability
Here we perform a nonparametric bootstrap to the karate club graph and the same selection of algorithms. For each instance, the set of vertices is resampled, the induced graph is obtained by taking the new set of vertices with the induced edges from the original graph, and the clustering algorithms are applied. Then, these results are compared to the induced original clusterings using several metrics: the variation of information (VI), normalized reduced mutual information (NRMI) and both adjusted and regular Rand index (Rand and adRand):
```{r}
b_karate <- boot_alg_list(g=karate, return_data=FALSE, R=9) 
b_karate
```

And the same for the stochastic block model graph:
```{r}
b_sbm <- boot_alg_list(g=g_sbm, return_data=FALSE, R=99)
b_sbm
```
We can clearly see that for all metrics, the results are much more stable, which makes sense because we created the sbm graph with very strong clusters.



